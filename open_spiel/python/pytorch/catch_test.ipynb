{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from open_spiel.python.algorithms import policy_gradient\n",
    "from open_spiel.python.algorithms import dqn\n",
    "import policy_gradient as pt_policy_gradient\n",
    "from open_spiel.python.environments import catch\n",
    "\n",
    "def _eval_agent(env, agent, num_episodes):\n",
    "  \"\"\"Evaluates `agent` for `num_episodes`.\"\"\"\n",
    "  rewards = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = env.reset()\n",
    "    episode_reward = 0\n",
    "    while not time_step.last():\n",
    "      agent_output = agent.step(time_step, is_evaluation=True)\n",
    "      time_step = env.step([agent_output.action])\n",
    "      episode_reward += time_step.rewards[0]\n",
    "    rewards += episode_reward\n",
    "  return rewards / num_episodes\n",
    "\n",
    "\n",
    "def test_tf(num_episodes, eval_interval, algorithm):\n",
    "  env = catch.Environment()\n",
    "  info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "  train_episodes = num_episodes\n",
    "\n",
    "  result = []\n",
    "  with tf.Session() as sess:\n",
    "    if algorithm in {\"rpg\", \"qpg\", \"rm\", \"a2c\"}:\n",
    "      agent = policy_gradient.PolicyGradient(\n",
    "          sess,\n",
    "          player_id=0,\n",
    "          info_state_size=info_state_size,\n",
    "          num_actions=num_actions,\n",
    "          loss_str=algorithm,\n",
    "          hidden_layers_sizes=[128, 128],\n",
    "          batch_size=128,\n",
    "          entropy_cost=0.01,\n",
    "          critic_learning_rate=0.1,\n",
    "          pi_learning_rate=0.1,\n",
    "          num_critic_before_pi=3)\n",
    "    elif algorithm == \"dqn\":\n",
    "      agent = dqn.DQN(\n",
    "          sess,\n",
    "          player_id=0,\n",
    "          state_representation_size=info_state_size,\n",
    "          num_actions=num_actions,\n",
    "          learning_rate=1e-3,\n",
    "          replay_buffer_capacity=10000,\n",
    "          hidden_layers_sizes=[32, 32],\n",
    "          epsilon_decay_duration=2000,  # 10% total data\n",
    "          update_target_network_every=250)\n",
    "    else:\n",
    "      raise ValueError(\"Algorithm not implemented!\")\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train agent\n",
    "    for ep in range(train_episodes):\n",
    "      time_step = env.reset()\n",
    "      while not time_step.last():\n",
    "        agent_output = agent.step(time_step)\n",
    "        action_list = [agent_output.action]\n",
    "        time_step = env.step(action_list)\n",
    "      # Episode is over, step agent with final info state.\n",
    "      agent.step(time_step)\n",
    "\n",
    "      if ep and ep % eval_interval == 0:\n",
    "        avg_return = _eval_agent(env, agent, 100)\n",
    "        result.append(avg_return)\n",
    "        #print(f\"{ep}:{avg_return}\")\n",
    "  return result\n",
    "\n",
    "def test_pt(num_episodes, eval_interval, algorithm):\n",
    "  \"\"\"Trains a DQN agent in the catch environment.\"\"\"\n",
    "  env = catch.Environment()\n",
    "  info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "  train_episodes = num_episodes\n",
    "\n",
    "  result = []\n",
    "  if algorithm in {\"rpg\", \"qpg\", \"rm\", \"a2c\"}:\n",
    "    agent = pt_policy_gradient.PolicyGradient(\n",
    "        player_id=0,\n",
    "        info_state_size=info_state_size,\n",
    "        num_actions=num_actions,\n",
    "        loss_str=algorithm,\n",
    "        hidden_layers_sizes=[128, 128],\n",
    "        batch_size=128,\n",
    "        entropy_cost=1e-3,\n",
    "        critic_learning_rate=0.1,\n",
    "        pi_learning_rate=0.1,\n",
    "        num_critic_before_pi=3)\n",
    "  else:\n",
    "    raise ValueError(\"Algorithm not implemented!\")\n",
    "  \n",
    "  # Train agent\n",
    "  for ep in range(train_episodes):\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      agent_output = agent.step(time_step)\n",
    "      action_list = [agent_output.action]\n",
    "      time_step = env.step(action_list)\n",
    "    # Episode is over, step agent with final info state.\n",
    "    agent.step(time_step)\n",
    "  \n",
    "    if ep and ep % eval_interval == 0:\n",
    "      avg_return = _eval_agent(env, agent, 100)\n",
    "      result.append(avg_return)\n",
    "      #print(f\"{ep}:{avg_return}\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = int(1e5)\n",
    "EVAL_INTERVAL = int(1e3)\n",
    "ALGORITHM = 'a2c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.56, -0.52, -0.52, -0.36, -0.44, -0.52, -0.46, -0.46, -0.52, -0.44, -0.48, -0.62, -0.4, -0.62, -0.46, -0.44, -0.56, -0.34, -0.48, -0.4, -0.4, -0.36, -0.44, -0.48, -0.48, -0.52, -0.52, -0.48, -0.34, -0.44, -0.44, -0.46, -0.54, -0.5, -0.3, -0.52, -0.36, -0.44, -0.38, -0.26, -0.46, -0.38, -0.42, -0.38, -0.24, -0.26, -0.32, -0.2, -0.42, -0.4, -0.4, -0.34, -0.28, -0.4, -0.2, -0.34, -0.22, -0.3, -0.14, -0.14, -0.3, -0.24, -0.34, -0.02, -0.14, -0.06, -0.16, -0.24, -0.1, -0.1, -0.04, 0.1, -0.1, 0.18, 0.12, -0.06, 0.16, 0.14, 0.16, 0.02, 0.0, 0.16, 0.28, 0.24, 0.36, 0.3, 0.18, 0.4, 0.28, 0.12, 0.28, 0.22, 0.26, 0.44, 0.44, 0.4, 0.58, 0.6, 0.42]\n",
      "[-0.5, -0.68, -0.68, -0.58, -0.48, -0.4, -0.42, -0.5, -0.18, -0.3, -0.48, -0.58, -0.54, -0.52, -0.46, -0.56, -0.6, -0.52, -0.4, -0.5, -0.58, -0.6, -0.5, -0.58, -0.42, -0.58, -0.54, -0.54, -0.28, -0.56, -0.48, -0.26, -0.52, -0.54, -0.46, -0.52, -0.32, -0.48, -0.36, -0.32, -0.38, -0.28, -0.52, -0.46, -0.46, -0.38, -0.38, -0.4, -0.38, -0.2, -0.34, -0.44, -0.32, -0.2, -0.1, -0.32, -0.16, -0.06, -0.18, -0.32, -0.26, -0.16, 0.04, -0.06, -0.18, -0.12, 0.02, 0.02, -0.14, -0.04, -0.08, -0.04, 0.04, -0.22, -0.08, 0.08, 0.0, 0.24, 0.0, 0.06, 0.0, -0.14, 0.02, 0.0, 0.06, -0.04, -0.04, 0.16, -0.1, 0.14, 0.02, 0.08, 0.12, 0.08, 0.12, 0.1, 0.06, 0.04, 0.06]\n",
      "[-0.62, -0.54, -0.52, -0.5, -0.58, -0.62, -0.52, -0.56, -0.6, -0.5, -0.56, -0.58, -0.54, -0.5, -0.5, -0.6, -0.44, -0.58, -0.44, -0.48, -0.54, -0.7, -0.56, -0.48, -0.48, -0.36, -0.28, -0.4, -0.44, -0.66, -0.48, -0.42, -0.5, -0.48, -0.6, -0.42, -0.64, -0.5, -0.5, -0.32, -0.28, -0.28, -0.34, -0.44, -0.34, -0.3, -0.4, -0.48, -0.38, -0.34, -0.48, -0.26, -0.28, -0.22, -0.22, -0.36, -0.16, -0.3, -0.3, -0.18, -0.26, -0.22, -0.24, -0.02, -0.16, -0.04, 0.02, -0.06, 0.08, 0.18, 0.16, 0.12, 0.16, 0.22, 0.14, 0.24, 0.18, 0.14, 0.16, 0.3, 0.38, 0.38, 0.3, 0.2, 0.32, 0.24, 0.46, 0.38, 0.4, 0.32, 0.4, 0.58, 0.46, 0.46, 0.52, 0.6, 0.6, 0.64, 0.64]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    res_tf = test_tf(NUM_EPISODES, EVAL_INTERVAL, ALGORITHM)\n",
    "    print(res_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.64, -0.48, -0.72, -0.44, -0.56, -0.62, -0.52, -0.44, -0.54, -0.48, -0.52, -0.56, -0.64, -0.44, -0.38, -0.56, -0.5, -0.42, -0.58, -0.38, -0.46, -0.54, -0.46, -0.52, -0.72, -0.4, -0.6, -0.38, -0.36, -0.5, -0.44, -0.44, -0.42, -0.38, -0.52, -0.36, -0.54, -0.36, -0.48, -0.38, -0.44, -0.4, -0.32, -0.4, -0.5, -0.44, -0.44, -0.32, -0.44, -0.42, -0.3, -0.22, -0.34, -0.5, -0.28, -0.46, -0.4, -0.04, -0.1, -0.26, -0.32, -0.28, -0.18, -0.36, -0.26, -0.1, -0.42, -0.22, -0.16, -0.12, -0.14, -0.28, -0.1, -0.08, -0.06, 0.0, 0.1, 0.02, 0.0, 0.04, 0.12, 0.06, 0.18, 0.12, 0.1, 0.34, 0.22, 0.2, 0.28, 0.22, 0.16, 0.04, 0.14, 0.28, 0.42, 0.42, 0.44, 0.4, 0.54]\n",
      "[-0.62, -0.48, -0.36, -0.56, -0.6, -0.6, -0.56, -0.5, -0.56, -0.56, -0.4, -0.26, -0.46, -0.5, -0.44, -0.5, -0.52, -0.38, -0.54, -0.5, -0.44, -0.56, -0.46, -0.3, -0.54, -0.48, -0.4, -0.42, -0.52, -0.46, -0.48, -0.5, -0.58, -0.38, -0.54, -0.38, -0.42, -0.46, -0.38, -0.4, -0.32, -0.38, -0.34, -0.44, -0.34, -0.38, -0.36, -0.24, -0.18, -0.18, -0.2, -0.18, -0.24, -0.24, -0.28, -0.18, -0.12, -0.3, -0.18, -0.2, -0.22, -0.06, -0.2, 0.08, 0.16, -0.02, 0.12, 0.28, 0.28, 0.08, 0.16, 0.28, 0.28, 0.28, 0.3, 0.3, 0.16, 0.24, 0.4, 0.24, 0.34, 0.36, 0.44, 0.54, 0.5, 0.54, 0.62, 0.52, 0.52, 0.48, 0.54, 0.56, 0.54, 0.56, 0.68, 0.68, 0.68, 0.68, 0.72]\n",
      "[-0.4, -0.56, -0.58, -0.46, -0.6, -0.54, -0.6, -0.46, -0.56, -0.46, -0.48, -0.32, -0.62, -0.48, -0.5, -0.32, -0.58, -0.44, -0.56, -0.34, -0.52, -0.58, -0.48, -0.6, -0.38, -0.5, -0.38, -0.3, -0.58, -0.52, -0.46, -0.3, -0.42, -0.36, -0.46, -0.46, -0.58, -0.4, -0.32, -0.46, -0.24, -0.66, -0.28, -0.38, -0.3, -0.4, -0.44, -0.46, -0.42, -0.38, -0.52, -0.34, -0.28, -0.14, -0.4, -0.2, -0.28, 0.0, -0.14, -0.34, -0.1, -0.14, -0.02, -0.24, -0.16, 0.18, -0.16, -0.02, 0.08, 0.1, 0.18, 0.18, -0.1, 0.22, 0.28, 0.1, 0.22, 0.22, 0.36, 0.16, 0.36, 0.4, 0.46, 0.56, 0.36, 0.34, 0.3, 0.44, 0.42, 0.54, 0.5, 0.68, 0.58, 0.74, 0.64, 0.66, 0.64, 0.74, 0.76]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    res_pt = test_pt(NUM_EPISODES, EVAL_INTERVAL, ALGORITHM)\n",
    "    print(res_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = int(1e5)\n",
    "EVAL_INTERVAL = int(1e3)\n",
    "ALGORITHM = 'dqn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1):\n",
    "    res_tf = test_tf(NUM_EPISODES, EVAL_INTERVAL, ALGORITHM)\n",
    "    print(res_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
