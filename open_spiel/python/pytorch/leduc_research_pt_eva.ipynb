{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_spiel.python import policy\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "import pyspiel\n",
    "\n",
    "import eva\n",
    "\n",
    "class JointPolicy(policy.Policy):\n",
    "  \"\"\"Joint policy to be evaluated.\"\"\"\n",
    "\n",
    "  def __init__(self, env, agents):\n",
    "    game = env.game\n",
    "    player_ids = list(range(len(agents)))\n",
    "    super(JointPolicy, self).__init__(game, player_ids)\n",
    "    self._agents = agents\n",
    "\n",
    "  def action_probabilities(self, state, player_id=None):\n",
    "    cur_player = state.current_player()\n",
    "    legal_actions = state.legal_actions(cur_player)\n",
    "    probs = self._agents[cur_player].action_probabilities(state)\n",
    "    return {action: probs[action] for action in legal_actions}\n",
    "\n",
    "\n",
    "def pt_main(game_name, num_episodes):\n",
    "  env_configs = {\"players\": 2}\n",
    "  env = rl_environment.Environment(game_name, **env_configs)\n",
    "  num_players = env.num_players\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "  state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  eva_agents = []\n",
    "  for player in range(num_players):\n",
    "    eva_agents.append(\n",
    "        eva.EVAAgent(\n",
    "            env,\n",
    "            player,\n",
    "            state_size,\n",
    "            num_actions,\n",
    "            batch_size=128,\n",
    "            learning_rate=0.01,\n",
    "            mixing_parameter=0.5,\n",
    "            memory_capacity=int(1e6),\n",
    "            discount_factor=1.0,\n",
    "            update_target_network_every=1000,\n",
    "            epsilon_start=0.06,\n",
    "            epsilon_end=0.001,\n",
    "            epsilon_decay_duration=int(1e6)))\n",
    "  \n",
    "  joint_policy = JointPolicy(env, eva_agents)\n",
    "  \n",
    "  result = []\n",
    "  for episode in range(num_episodes):\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "      conv = exploitability.nash_conv(env.game, joint_policy)\n",
    "      result.append(conv)\n",
    "      print(\"Episode:%s - NashConv: %s\" %(episode+1, conv))\n",
    "      \n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "      current_player = time_step.observations[\"current_player\"]\n",
    "      current_agent = eva_agents[current_player]\n",
    "      step_out = current_agent.step(time_step)\n",
    "      time_step = env.step([step_out.action])\n",
    "        \n",
    "    for agent in eva_agents:\n",
    "      agent.step(time_step)\n",
    "        \n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1000 - NashConv: 4.816378315359527\n",
      "Episode:2000 - NashConv: 5.183435771931083\n",
      "Episode:3000 - NashConv: 4.981805847350352\n",
      "Episode:4000 - NashConv: 4.959138290012623\n",
      "Episode:5000 - NashConv: 4.987403549910527\n",
      "Episode:6000 - NashConv: 4.894065221510903\n",
      "Episode:7000 - NashConv: 5.087053143228525\n",
      "Episode:8000 - NashConv: 5.001063352058948\n",
      "Episode:9000 - NashConv: 4.574781550661821\n",
      "Episode:10000 - NashConv: 4.45628410467555\n",
      "[4.816378315359527, 5.183435771931083, 4.981805847350352, 4.959138290012623, 4.987403549910527, 4.894065221510903, 5.087053143228525, 5.001063352058948, 4.574781550661821, 4.45628410467555]\n",
      "Episode:1000 - NashConv: 4.762049732195072\n",
      "Episode:2000 - NashConv: 5.220065921048683\n",
      "Episode:3000 - NashConv: 5.434335654150852\n",
      "Episode:4000 - NashConv: 5.148536129794721\n",
      "Episode:5000 - NashConv: 4.927764855547408\n",
      "Episode:6000 - NashConv: 4.880683280302989\n",
      "Episode:7000 - NashConv: 4.770471931455553\n",
      "Episode:8000 - NashConv: 4.908578138855047\n",
      "Episode:9000 - NashConv: 4.944229230221213\n",
      "Episode:10000 - NashConv: 4.990821043836672\n",
      "[4.762049732195072, 5.220065921048683, 5.434335654150852, 5.148536129794721, 4.927764855547408, 4.880683280302989, 4.770471931455553, 4.908578138855047, 4.944229230221213, 4.990821043836672]\n",
      "Episode:1000 - NashConv: 4.738434718539721\n",
      "Episode:2000 - NashConv: 4.740359240222743\n",
      "Episode:3000 - NashConv: 4.715566360673841\n",
      "Episode:4000 - NashConv: 4.695352025473474\n",
      "Episode:5000 - NashConv: 4.673802467801391\n",
      "Episode:6000 - NashConv: 4.673510013744725\n",
      "Episode:7000 - NashConv: 4.674157125854695\n",
      "Episode:8000 - NashConv: 4.724951769213946\n",
      "Episode:9000 - NashConv: 4.7457643793637425\n",
      "Episode:10000 - NashConv: 4.743087811560461\n",
      "[4.738434718539721, 4.740359240222743, 4.715566360673841, 4.695352025473474, 4.673802467801391, 4.673510013744725, 4.674157125854695, 4.724951769213946, 4.7457643793637425, 4.743087811560461]\n"
     ]
    }
   ],
   "source": [
    "pt_result = []\n",
    "for _ in range(3):\n",
    "    result = pt_main('leduc_poker', 10000)\n",
    "    print(result)\n",
    "    pt_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
