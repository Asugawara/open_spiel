{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sugawara/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from open_spiel.python import policy\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import eva\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "import pyspiel\n",
    "\n",
    "class JointPolicy(policy.Policy):\n",
    "  \"\"\"Joint policy to be evaluated.\"\"\"\n",
    "\n",
    "  def __init__(self, env, agents):\n",
    "    game = env.game\n",
    "    player_ids = list(range(len(agents)))\n",
    "    super(JointPolicy, self).__init__(game, player_ids)\n",
    "    self._agents = agents\n",
    "\n",
    "  def action_probabilities(self, state, player_id=None):\n",
    "    cur_player = state.current_player()\n",
    "    legal_actions = state.legal_actions(cur_player)\n",
    "    probs = self._agents[cur_player].action_probabilities(state)\n",
    "    return {action: probs[action] for action in legal_actions}\n",
    "\n",
    "\n",
    "def tf_main(game_name, num_episodes):\n",
    "  env_configs = {\"players\": 2}\n",
    "  env = rl_environment.Environment(game_name, **env_configs)\n",
    "  num_players = env.num_players\n",
    "  num_actions = env.action_spec()[\"num_actions\"]\n",
    "  state_size = env.observation_spec()[\"info_state\"][0]\n",
    "  eva_agents = []\n",
    "  with tf.Session() as sess:\n",
    "    for player in range(num_players):\n",
    "      eva_agents.append(\n",
    "          eva.EVAAgent(\n",
    "              sess,\n",
    "              env,\n",
    "              player,\n",
    "              state_size,\n",
    "              num_actions,\n",
    "              batch_size=128,\n",
    "              learning_rate=0.01,\n",
    "              mixing_parameter=0.5,\n",
    "              memory_capacity=int(1e6),\n",
    "              discount_factor=1.0,\n",
    "              update_target_network_every=1000,\n",
    "              epsilon_start=0.06,\n",
    "              epsilon_end=0.001,\n",
    "              epsilon_decay_duration=int(1e6)))\n",
    "    \n",
    "    joint_policy = JointPolicy(env, eva_agents)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    result = []\n",
    "    for episode in range(num_episodes):\n",
    "      if (episode + 1) % 1000 == 0:\n",
    "        conv = exploitability.nash_conv(env.game, joint_policy)\n",
    "        result.append(conv)\n",
    "        print(\"Episode:%s - NashConv: %s\" %(episode+1, conv))\n",
    "        \n",
    "      time_step = env.reset()\n",
    "      while not time_step.last():\n",
    "        current_player = time_step.observations[\"current_player\"]\n",
    "        current_agent = eva_agents[current_player]\n",
    "        step_out = current_agent.step(time_step)\n",
    "        time_step = env.step([step_out.action])\n",
    "        \n",
    "      for agent in eva_agents:\n",
    "        agent.step(time_step)\n",
    "        \n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1000 - NashConv: 4.720580705953815\n",
      "Episode:2000 - NashConv: 4.749483997465015\n",
      "Episode:3000 - NashConv: 4.762153759840439\n",
      "Episode:4000 - NashConv: 4.614934653791604\n",
      "Episode:5000 - NashConv: 4.694948923098359\n",
      "Episode:6000 - NashConv: 4.58856208386835\n",
      "Episode:7000 - NashConv: 4.500648809481751\n",
      "Episode:8000 - NashConv: 4.36771227475684\n",
      "Episode:9000 - NashConv: 4.4433707451544375\n",
      "Episode:10000 - NashConv: 4.453912521985062\n",
      "[4.720580705953815, 4.749483997465015, 4.762153759840439, 4.614934653791604, 4.694948923098359, 4.58856208386835, 4.500648809481751, 4.36771227475684, 4.4433707451544375, 4.453912521985062]\n"
     ]
    }
   ],
   "source": [
    "tf_result = []\n",
    "for _ in range(1):\n",
    "    result = tf_main('leduc_poker', 10000)\n",
    "    print(result)\n",
    "    tf_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1000 - NashConv: 4.775832579685689\n",
      "Episode:2000 - NashConv: 4.990296873522524\n"
     ]
    }
   ],
   "source": [
    "tf_result = []\n",
    "for _ in range(1):\n",
    "    result = tf_main('leduc_poker', 10000)\n",
    "    print(result)\n",
    "    tf_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
